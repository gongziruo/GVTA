from gssm import *
import numpy as np
import torch
import logging
import sys
LOGGER = logging.getLogger(__name__)
from torch.utils.data import Dataset, DataLoader
from torch.optim import Adam
import torch.nn as nn
import matplotlib.pyplot as plt

def parse_args():
    import argparse
    parser = argparse.ArgumentParser(description="vta agr parser")
    parser.add_argument('--seed', type=int, default=111)

    # data size
    parser.add_argument('--batch-size', type=int, default=5)
    parser.add_argument('--seq-size', type=int, default=500)
    parser.add_argument('--init-size', type=int, default=0)

    # model size
    parser.add_argument('--state-size', type=int, default=4)
    parser.add_argument('--belief-size', type=int, default=16)
    parser.add_argument('--num-layers', type=int, default=1)

    # observation distribution
    parser.add_argument('--obs-std', type=float, default=1.0)
    parser.add_argument('--obs-bit', type=int, default=5)

    # optimization
    parser.add_argument('--learn-rate', type=float, default=0.0001)
    parser.add_argument('--grad-clip', type=float, default=1.0)
    parser.add_argument('--max-iters', type=int, default=400)#100000

    # subsequence prior params
    parser.add_argument('--seg-num', type=int, default=4)#
    parser.add_argument('--seg-len', type=int, default=5)#

    # gumbel params
    parser.add_argument('--max-beta', type=float, default=1.0)
    parser.add_argument('--min-beta', type=float, default=0.1)
    parser.add_argument('--beta-anneal', type=float, default=100)

    # log dir
    parser.add_argument('--log-dir', type=str, default='./asset/log/')
    return parser.parse_args()


class MazeDataset(Dataset):
    def __init__(self, length, partition, path):
        self.partition = partition

        if self.partition == 'train':
            dataset = np.load(path)
            #print(np.shape(dataset['data']))
            self.state = dataset['data'].transpose(1, 0).reshape(-1,500,1,2,2)
        else:
            dataset = np.load(path)
            self.state = dataset['data'].transpose(1, 0).reshape(-1,500,1,2,2)

        self.length = length#12
        self.full_length = self.state.shape[1]#100

    def __len__(self):
        return self.state.shape[0]#10

    def __getitem__(self, index):
        state=self.state[index, 0:self.length].astype(np.float32)
        return state

def full_dataloader(seq_size, init_size, batch_size, test_size=5):#box2_1000x10x32x32x3.npz
    train_loader = MazeDataset(length=seq_size + init_size * 2, partition='train', path='./datatrain4000000_2_500.npz')
    test_loader = MazeDataset(length=seq_size + init_size * 2, partition='test', path='./datatest2000000_2_5000.npz')
    train_loader = DataLoader(dataset=train_loader, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(dataset=test_loader, batch_size=test_size, shuffle=False)
    return train_loader,test_loader

if __name__ == '__main__':
    # parse arguments
    args = parse_args()

    # fix seed
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    torch.cuda.manual_seed_all(args.seed)
    torch.backends.cudnn.deterministic = True

    # set logger
    log_format = '[%(asctime)s] %(message)s'
    logging.basicConfig(level=logging.INFO, format=log_format, stream=sys.stderr)

    # set size
    seq_size = args.seq_size  # 20
    init_size = args.init_size  # 0

    # set device as gpu
    #device = torch.device("cuda:3" if torch.cuda.is_available() else "cpu")
    device = torch.device('cuda', 3)

    # load dataset
    # load dataset
    train_loader, test_loader = full_dataloader(seq_size, init_size, args.batch_size)
    LOGGER.info('Dataset loaded')

    # init models
    model = lightningEnvModel(belief_size=args.belief_size,
                     state_size=args.state_size,
                     num_layers=args.num_layers,
                     max_seg_len=args.seg_len,
                     max_seg_num=args.seg_num).to(device)
    model.load_state_dict(torch.load('./model.pt'))
    for parameters in model.parameters():
        print(parameters)
    exit()
    model.load_state_dict(torch.load('./model-5000u.pt'))
    for parameters in model.parameters():
        print(parameters)
    exit()
    print("model",model)
    LOGGER.info('Model initialized')
    # init optimizer

    optimizer = Adam(params=model.parameters(),
                     lr=args.learn_rate, amsgrad=True)

    # test data
    pre_test_full_data_list = iter(test_loader).next()
    pre_test_full_data_list = pre_test_full_data_list.to(device)

    #save
    trainloss=[]
    testloss=[]
    train_mask=[]
    test_mask=[]
    train_F_measure=[]
    test_F_measure=[]
    train_Accuracy=[]
    test_Accuracy=[]
    train_x=[]
    test_x=[]
    k=0#index
    j=0#index
    # 指标
    # path = './mask_list.npz'
    # dataset = np.load(path)
    # mask_target = dataset['data'].reshape(5,20,1)
    # for each enpoch
    enpoch=0
    while enpoch < 20:
        # for each iter
        b_idx = 0
        while b_idx <args.max_iters:  # 10000
            # for each batch
            for train_obs_list in train_loader:  # (5,20,1,2,2)
                b_idx += 1  # 10000
                # mask temp annealing
                if args.beta_anneal:  # 100
                    model.state_model.mask_beta = (args.max_beta - args.min_beta) * 0.999 ** (
                            b_idx / args.beta_anneal) + args.min_beta  # 0.9*0.999^(b_idx / args.beta_anneal)+0.1
                else:
                    model.state_model.mask_beta = args.max_beta  # 1.0
                # train time #
                # get input data #
                train_obs_list = train_obs_list.to(device)  #
                # run model with train mode
                model.train()
                # zero the parameter gradients
                optimizer.zero_grad()
                #10*8*8
                results = model(train_obs_list, seq_size, init_size,args.obs_std)  # train_obs_list in train_loader,20,5,1
                # get train loss and backward update
                train_total_loss = results['train_loss']
                train_total_loss.backward()
                if args.grad_clip > 0.0:
                    nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)
                optimizer.step()
                # log
                #if b_idx % 10 == 0:
                    #loss
                log_str, log_data,log_loss,mask_data = log_train(results, b_idx+enpoch*100)
                log_loss=log_loss.cpu().detach().numpy()
                trainloss.append(log_loss)
                train_mask.append(mask_data.cpu().detach().numpy())
                train_x.append(k)
                k+=1
                #指标
                boundary_data_list = results['mask_data']
                boundary_data_list = boundary_data_list.cpu().detach().numpy()
                # sum_mask=boundary_data_list+mask_target
                # sum_mask=sum_mask.reshape(-1)
                # Positive=[1 if n==2 else 0 for n in sum_mask]
                # Negetive=[1 if n==0 else 0 for n in sum_mask]
                # TP=sum(Positive)
                # TN=sum(Negetive)
                # boundary_data_list=boundary_data_list.reshape(-1)
                # P=sum(boundary_data_list)
                # Precision=TP/P
                # Recall=TP/20
                # train_Accuracy.append((TP+TN)/100)
                # if((Recall + Precision)==0):
                #     print("exit",Recall, Precision)
                #     #train_F_measure.append(0)
                # else:
                #     train_F_measure.append(Recall * Precision * 2 / (Recall + Precision))
                #     sample_prob = results['sample_prob']
                #     print("train_sample_prob", sample_prob.shape, sample_prob)
                LOGGER.info(log_str, *log_data)
        enpoch += 1
    #画图
    torch.save(model.state_dict(),'./model-5000u.pt')
    plt.figure("loss")
    plt.plot(train_x, trainloss, color='red', linewidth=2.0, linestyle='--')
    # plt.plot(test_x, testloss, color='blue', linewidth=3.0, linestyle='-.')
    plt.savefig("train-500u-loss-learn_reat00001-F-5000s-02")
    # plt.figure("F_measure")
    # plt.plot(train_x, train_F_measure, color='red', linewidth=2.0, linestyle='--')
    # # plt.plot(test_x, test_F_measure, color='blue', linewidth=3.0, linestyle='-.')
    # plt.savefig("train-F_measure-learn_reat00001-F-5000s-02")
    # plt.figure("Accuracy")
    # plt.plot(train_x, train_Accuracy, color='red', linewidth=2.0, linestyle='--')
    # # plt.plot(test_x, test_Accuracy, color='blue', linewidth=3.0, linestyle='-.')
    # plt.savefig("train-Accuracy-learn_reat00001-F-5000s-02")
    plt.figure("mask")
    plt.plot(train_x, train_mask, color='red', linewidth=2.0, linestyle='--')
    # plt.plot(test_x, test_mask, color='blue', linewidth=3.0, linestyle='-.')
    plt.savefig("train-500u-mask-learn_reat00001-F-5000s-02")
